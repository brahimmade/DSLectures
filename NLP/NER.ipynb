{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "### Build, train and deploy state of the art models powered by the reference open source in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification, pipeline\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to watch out:\n",
    "- Models and Dataset can be very large and so can consume a lot of RAM / disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "https://huggingface.co/datasets\n",
    "- Hugging Face hosts over 5000 datasets for text, and they are tagged by language, size and task (classification, question answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each Dataset should have a filled out datacard, explaining what the dataset contains:\n",
    "- https://huggingface.co/datasets/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (C:\\Users\\Adrian\\.cache\\huggingface\\datasets\\ag_news\\default\\0.0.0\\bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 124.99it/s]\n"
     ]
    }
   ],
   "source": [
    "ag_news = load_dataset(\"ag_news\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a specific article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news[\"train\"][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained Models out of the box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What hugging face is most well known about is it's collection of state of the art models\n",
    "- They have 47,000 pretrained models that you can use\n",
    "- Supporting over 120 model architectures\n",
    "- We will need the tokenizer of the model (Making text machine readable, and different models have different formats they expect)\n",
    "- And the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Jean-Baptiste/roberta-large-ner-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be customised based on the model used, as the entities will be different and the output format often varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(task=\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def apply_ner(text: str):\n",
    "    classifications = ner_pipeline(text, aggregation_strategy=\"simple\")\n",
    "\n",
    "    entities = []\n",
    "    for i in range(len(classifications)):\n",
    "        if classifications[i][\"entity_group\"] not in [\"O\", \"MISC\"]:\n",
    "            entities.append(\n",
    "                (\n",
    "                    classifications[i][\"entity_group\"],\n",
    "                    classifications[i][\"start\"],\n",
    "                    classifications[i][\"end\"],\n",
    "                )\n",
    "            )\n",
    "    spacy_display = {}\n",
    "    spacy_display[\"ents\"] = []\n",
    "    spacy_display[\"text\"] = text\n",
    "    spacy_display[\"title\"] = None\n",
    "\n",
    "    for entity in classifications:\n",
    "        spacy_display[\"ents\"].append(\n",
    "            {\n",
    "                \"start\": entity[\"start\"],\n",
    "                \"end\": entity[\"end\"],\n",
    "                \"label\": entity[\"entity_group\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    entity_list = [\"PER\", \"LOC\", \"ORG\", \"MISC\"]\n",
    "    colors = {\n",
    "        \"PER\": \"#85DCDF\",\n",
    "        \"LOC\": \"#DF85DC\",\n",
    "        \"ORG\": \"#DCDF85\",\n",
    "        \"MISC\": \"#85ABDF\",\n",
    "    }\n",
    "    html = spacy.displacy.render(\n",
    "        spacy_display,\n",
    "        style=\"ent\",\n",
    "        minify=True,\n",
    "        manual=True,\n",
    "        options={\"ents\": entity_list, \"colors\": colors},\n",
    "    )\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = ag_news[\"train\"][0][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><mark class=\"entity\" style=\"background: #DF85DC; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Wall St.<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span></mark> Bears Claw Back Into the Black (<mark class=\"entity\" style=\"background: #DCDF85; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Reuters<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span></mark>) <mark class=\"entity\" style=\"background: #DCDF85; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Reuters<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span></mark> - Short-sellers, <mark class=\"entity\" style=\"background: #DF85DC; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Wall Street<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span></mark>'s dwindling\\band of ultra-cynics, are seeing green again<mark class=\"entity\" style=\"background: #DF85DC; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">.<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span></mark></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_ner(example_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions.\n",
    "\n",
    "0: O\n",
    "1: B-corporation\n",
    "2: I-corporation\n",
    "3: B-creative-work\n",
    "4: I-creative-work\n",
    "5: B-group\n",
    "6: I-group\n",
    "7: B-location\n",
    "8: I-location\n",
    "9: B-person\n",
    "10: I-person\n",
    "11: B-product\n",
    "12: I-product\n",
    "\n",
    "B- indicates the beginning of an entity.\n",
    "I- indicates a token is contained inside the same entity (e.g., the State token is a part of an entity like Empire State Building)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wnut_17 (C:\\Users\\Adrian\\.cache\\huggingface\\datasets\\wnut_17\\wnut_17\\1.0.0\\077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 535.06it/s]\n"
     ]
    }
   ],
   "source": [
    "wnut = load_dataset(\"wnut_17\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Adrian\\miniconda3\\envs\\kaggle\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(\n",
    "            batch_index=i\n",
    "        )  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif (\n",
    "                word_idx != previous_word_idx\n",
    "            ):  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Adrian\\.cache\\huggingface\\datasets\\wnut_17\\wnut_17\\1.0.0\\077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9\\cache-99fd98fcc80797d7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Adrian\\.cache\\huggingface\\datasets\\wnut_17\\wnut_17\\1.0.0\\077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9\\cache-503a40c710afefa3.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Adrian\\.cache\\huggingface\\datasets\\wnut_17\\wnut_17\\1.0.0\\077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9\\cache-5aaa835acfced39a.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, bs = 5e-5, 16\n",
    "wd, epochs = 0.01, 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"/outputs\",\n",
    "    learning_rate=lr,\n",
    "    warmup_steps=35,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs * 2,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=wd,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"no\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Annealing: https://paperswithcode.com/method/cosine-annealing\n",
    "Mixed Precision TrainingL https://paperswithcode.com/paper/mixed-precision-training (fp16=True)\n",
    "Warmup: Train a few batches with low lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to explore:\n",
    "- Hyperparameter tuning\n",
    "- Cross fold validation & ensembling\n",
    "- Try Different Optimizers: Default is AdamW\n",
    "- Try differnt loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\Adrian\\miniconda3\\envs\\kaggle\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3394\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 426\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 213/426 [00:31<00:27,  7.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1287\n",
      "  Batch size = 32\n",
      "                                                 \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 214/426 [00:33<03:03,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21272608637809753, 'eval_runtime': 2.4298, 'eval_samples_per_second': 529.678, 'eval_steps_per_second': 16.874, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426/426 [01:02<00:00,  7.82it/s]The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DebertaV2ForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1287\n",
      "  Batch size = 32\n",
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426/426 [01:04<00:00,  7.82it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426/426 [01:04<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22114568948745728, 'eval_runtime': 2.341, 'eval_samples_per_second': 549.769, 'eval_steps_per_second': 17.514, 'epoch': 2.0}\n",
      "{'train_runtime': 64.7783, 'train_samples_per_second': 104.788, 'train_steps_per_second': 6.576, 'train_loss': 0.245234296915117, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=426, training_loss=0.245234296915117, metrics={'train_runtime': 64.7783, 'train_samples_per_second': 104.788, 'train_steps_per_second': 6.576, 'train_loss': 0.245234296915117, 'epoch': 2.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\n",
      "Configuration saved in model\\config.json\n",
      "Model weights saved in model\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\tokenizer_config.json\n",
      "Special tokens file saved in model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./model\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"./model\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ./model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DebertaV2ForTokenClassification.\n",
      "\n",
      "All the weights of DebertaV2ForTokenClassification were initialized from the model checkpoint at ./model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForTokenClassification for predictions without further training.\n",
      "loading file ./model\\spm.model\n",
      "loading file ./model\\tokenizer.json\n",
      "loading file ./model\\added_tokens.json\n",
      "loading file ./model\\special_tokens_map.json\n",
      "loading file ./model\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"./model\")\n",
    "ner_tok = AutoTokenizer.from_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(task=\"ner\", model=ner_model, tokenizer=ner_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ner(text: str):\n",
    "    classifications = ner_pipeline(text)\n",
    "\n",
    "    ner_map = {\n",
    "        \"LABEL_0\": \"O\",\n",
    "        \"LABEL_1\": \"B-corporation\",\n",
    "        \"LABEL_2\": \"I-corporation\",\n",
    "        \"LABEL_3\": \"B-creative-work\",\n",
    "        \"LABEL_4\": \"I-creative-work\",\n",
    "        \"LABEL_5\": \"B-group\",\n",
    "        \"LABEL_6\": \"I-group\",\n",
    "        \"LABEL_7\": \"B-location\",\n",
    "        \"LABEL_8\": \"I-location\",\n",
    "        \"LABEL_9\": \"B-person\",\n",
    "        \"LABEL_10\": \"I-person\",\n",
    "        \"LABEL_11\": \"B-product\",\n",
    "        \"LABEL_12\": \"I-product\",\n",
    "    }\n",
    "    spacy_display = {}\n",
    "    spacy_display[\"ents\"] = []\n",
    "    spacy_display[\"text\"] = \" \".join(text)\n",
    "    spacy_display[\"title\"] = None\n",
    "\n",
    "    entity_start = 0\n",
    "    for i in range(len(classifications)):\n",
    "\n",
    "        if classifications[i][0][\"entity\"] != \"label_0\":\n",
    "            if ner_map[classifications[i][0][\"entity\"]][0] == \"B\":\n",
    "                j = i + 1\n",
    "                entity_end = entity_start + classifications[i][0][\"end\"] + 1\n",
    "                while (\n",
    "                    j < len(classifications)\n",
    "                    and ner_map[classifications[j][0][\"entity\"]][0] == \"I\"\n",
    "                ):\n",
    "                    j += 1\n",
    "                    entity_end = entity_end + classifications[j][0][\"end\"] + 1\n",
    "                spacy_display[\"ents\"].append(\n",
    "                    {\n",
    "                        \"start\": entity_start,\n",
    "                        \"end\": entity_end,\n",
    "                        \"label\": ner_map[classifications[i][0][\"entity\"]].split(\"-\")[1],\n",
    "                    }\n",
    "                )\n",
    "        entity_start = entity_start + classifications[i][0][\"end\"] + 1\n",
    "\n",
    "    entity_list = [\n",
    "        \"corporation\",\n",
    "        \"creative-work\",\n",
    "        \"group\",\n",
    "        \"location\",\n",
    "        \"person\",\n",
    "        \"product\",\n",
    "    ]\n",
    "    colors = {\n",
    "        \"corporation\": \"lightblue\",\n",
    "        \"creative-work\": \"lightyellow\",\n",
    "        \"group\": \"lightgreen\",\n",
    "        \"location\": \"lightred\",\n",
    "        \"person\": \"orange\",\n",
    "        \"product\": \"black\",\n",
    "    }\n",
    "    html = spacy.displacy.render(\n",
    "        spacy_display,\n",
    "        style=\"ent\",\n",
    "        minify=True,\n",
    "        manual=True,\n",
    "        options={\"ents\": entity_list, \"colors\": colors},\n",
    "    )\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wnut[\"validation\"][2][\"tokens\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">All I ' ve been doing is BINGE watchin<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">g Ric<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">person</span></mark>k an<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">d Mort<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">person</span></mark>y ðŸ˜‚</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_ner(text)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6b7447e65a7d1bc4d4bc0d5ff62fe5925604424412d8ca4c10380051a18e2f7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
