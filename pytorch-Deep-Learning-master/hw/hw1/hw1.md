# Homework 1: Backpropagation

### 1.1 Two Layer Neural Nets
Neural net architecture
$$
Linear_1 \rightarrow f \rightarrow Linear_2 \rightarrow g
$$

where $linear_{i}(x) = W^{(i)}x + b^{(i)}$ is the $i$-th affine transformation and $f,g$ are element wise nonlinear activation functions. When an input $x \in \mathbb{R}$ is fed to the network, $ \hat{y} \in \mathbb{R}^K$ is obtained as the output.

### 1.2 Regression Task

a)